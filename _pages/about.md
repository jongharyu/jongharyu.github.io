---
layout: about
title: About
permalink: /
subtitle: Postdoctoral Associate at MIT

profile:
  align: right
  image: jon.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Room 36-677</p>
    <p>50 Vassar St</p>
    <p>Cambridge, MA 02139</p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

[//]: # '[//]: <span style="font-weight:bold"><mark>'
[//]: # "[//]: "
[//]: # "[//]: </mark></span>"

I am currently a postdoctoral associate at MIT, hosted by [Gregory W. Wornell](http://allegro.mit.edu/~gww/).
Prior to joining MIT, I received my Ph.D. in [Electrical Engineering](https://ece.ucsd.edu/) from [UC San Diego](https://ucsd.edu/),
where I was fortunate to be advised by [Young-Han Kim](https://web.eng.ucsd.edu/~yhk/) and [Sanjoy Dasgupta](https://cseweb.ucsd.edu/~dasgupta/).
My graduate study was generously supported by the [Kwanjeong Educational Foundation](http://www.ikef.or.kr/).
Before the graduate study, I received my B.S. in [Electrical and Computer Engineering](https://ece.snu.ac.kr/en) and B.S. in [Mathematical Sciences](https://www.math.snu.ac.kr/) (with minor in [Physics](https://physics.snu.ac.kr/en)) with the highest distinction from [Seoul National University](https://en.snu.ac.kr) in 2015.

In general, I aim to develop principled and practical algorithms for machine learning and data science.
My recent research topics include:

- **Designing algorithms for large-scale problems from first principles**
  - efficient parametric operator SVD for large-scale problems ([NeuralSVD [ICML2024]](http://arxiv.org/abs/2402.03655))
  - efficient small-k-nearest-neighbors algorithms [[TIT2022]](http://arxiv.org/abs/1805.08342), [[arXiv]](http://arxiv.org/abs/2202.02464)
- **Learning with efficient & reliable uncertainty quantification** 
  - tight time-uniform confidence sets [[TIT2024]](http://arxiv.org/abs/2207.12382), [[ICML2024]](http://arxiv.org/abs/2402.03683)
  - identifying pitfalls of evidential deep learning [[NeurIPS2024](http://arxiv.org/abs/2402.06160)]
- **Information-theoretic tools for machine learning and data science**
  - from universal gambling to time-uniform confidence sets [[TIT2024]](http://arxiv.org/abs/2207.12382), [[ICML2024]](http://arxiv.org/abs/2402.03683)
  - from universal compression to parameter-free online optimization [[AISTATS2022]](http://arxiv.org/abs/2202.02406)
  - information-theoretic common representation learning ([variational Wyner model [arXiv]](http://arxiv.org/abs/1905.10945))
- **Unifying principles in machine learning**
  - unified view on density functional estimation with fixed-k-NNs [[TIT2022]](http://arxiv.org/abs/1805.08342)
  - unifying evidential deep learning methods for uncertainty quantification [[NeurIPS2024](http://arxiv.org/abs/2402.06160)]
  - unifying principles for fitting unnormalized distributions via noise-contrastive estimation [[arXiv](http://arxiv.org/abs/2409.18209)]

As an information theorist by training, I enjoy doing research by simplifying intricate ideas, unifying concepts, and generalizing them to address complex problems.

Check out my [resume](/resume) for more information.
