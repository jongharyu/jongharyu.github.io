---
layout: about
title: about
permalink: /
subtitle: Postdoctoral Associate at MIT

profile:
  align: right
  image: jon.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Room 36-677</p>
    <p>50 Vassar St</p>
    <p>Cambridge, MA 02139</p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

[//]: # '[//]: <span style="font-weight:bold"><mark>'
[//]: # "[//]: "
[//]: # "[//]: </mark></span>"

I am currently a postdoctoral associate at MIT, hosted by [Gregory W. Wornell](http://allegro.mit.edu/~gww/).
Prior to joining MIT, I received my Ph.D. in [Electrical Engineering](https://ece.ucsd.edu/) from [UC San Diego](https://ucsd.edu/),
where I was fortunate to be advised by [Young-Han Kim](https://web.eng.ucsd.edu/~yhk/) and [Sanjoy Dasgupta](https://cseweb.ucsd.edu/~dasgupta/).
My graduate study was generously supported by the [Kwanjeong Educational Foundation](http://www.ikef.or.kr/).
Before the graduate study, I received my B.S. in [Electrical and Computer Engineering](https://ece.snu.ac.kr/en) and B.S. in [Mathematical Sciences](https://www.math.snu.ac.kr/) (with minor in [Physics](https://physics.snu.ac.kr/en)) with the highest distinction from [Seoul National University](https://en.snu.ac.kr) in 2015.

In general, I aim to develop principled and practical algorithms for machine learning and data science.
My recent research topics include:

- **Scalable parametric spectral decomposition methods**
  - efficient parametric operator SVD ([NeuralSVD [ICML2024a]](http://arxiv.org/abs/2402.03655))
- **New generative modeling techniques**
  - new training framework for one-step generative models ([Score-of-Mixture Training [arXiv2025a]](https://arxiv.org/abs/2502.09609))
  - unifying principles for fitting unnormalized distributions [[arXiv2024](http://arxiv.org/abs/2409.18209)]
- **Efficient & reliable uncertainty quantification techniques** 
  - universal-gambling-based time-uniform confidence sets [[TIT2024]](http://arxiv.org/abs/2207.12382), [[ICML2024b]](http://arxiv.org/abs/2402.03683), and applications [[arXiv2025b]](https://arxiv.org/abs/2502.10826)
  - identifying pitfalls of evidential deep learning [[NeurIPS2024](http://arxiv.org/abs/2402.06160)]

- **Nonparametric methods**
  - unified view on density functional estimation with fixed-k-NNs [[TIT2022]](http://arxiv.org/abs/1805.08342)
  - efficient small-k-nearest-neighbors algorithms [[arXiv2022]](http://arxiv.org/abs/2202.02464)
  
[//]: # (- **Information-theoretic tools for machine learning**)

[//]: # (  - from universal compression to parameter-free online optimization [[AISTATS2022]]&#40;http://arxiv.org/abs/2202.02406&#41;)

[//]: # (  - information-theoretic common representation learning &#40;[variational Wyner model [arXiv]]&#40;http://arxiv.org/abs/1905.10945&#41;&#41;)

[//]: # (  - unifying evidential deep learning methods for uncertainty quantification [[NeurIPS2024]&#40;http://arxiv.org/abs/2402.06160&#41;])

As an information theorist by training, I enjoy doing research by simplifying intricate ideas, unifying concepts, and generalizing them to address complex problems.

Check out my [resume](/resume) for more information.
