---
layout: about
title: about
permalink: /
subtitle: Postdoctoral Associate at MIT

profile:
  align: right
  image: jon.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Room 36-677</p>
    <p>50 Vassar St</p>
    <p>Cambridge, MA 02139</p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am currently a postdoctoral associate at MIT, hosted by [Gregory W. Wornell](http://allegro.mit.edu/~gww/).
Prior to joining MIT, I received my Ph.D. in [Electrical Engineering](https://ece.ucsd.edu/) from [UC San Diego](https://ucsd.edu/),
where I was fortunate to be advised by [Young-Han Kim](https://web.eng.ucsd.edu/~yhk/) and [Sanjoy Dasgupta](https://cseweb.ucsd.edu/~dasgupta/).
My graduate study was generously supported by the [Kwanjeong Educational Foundation](http://www.ikef.or.kr/).
Before the graduate study, I received my B.S. in [Electrical and Computer Engineering](https://ece.snu.ac.kr/en) and B.S. in [Mathematical Sciences](https://www.math.snu.ac.kr/) (with minor in [Physics](https://physics.snu.ac.kr/en)) with the highest distinction from [Seoul National University](https://en.snu.ac.kr) in 2015.

I am interested in a broad range of topics related to learning from data, both in theory and practice.
My recent research focuses on:

- **New machine learning techniques for scalable scientific simulation**
  - a parametric framework for operator SVD ([NeuralSVD [ICML2024a]](http://arxiv.org/abs/2402.03655))  
  - variations and applications (work in progress)

- **New techniques for probabilistic (generative) models**
  - an efficient framework for training one-step, high-quality generative models ([Score-of-Mixture Training [ICML2025a]](https://arxiv.org/abs/2502.09609))  
  - unifying principles for learning with energy-based models [[ICML2025b](http://arxiv.org/abs/2409.18209)]

- **New techniques and perspectives for uncertainty quantification**
  - universal gambling-based time-uniform confidence sets [[TIT2024]](http://arxiv.org/abs/2207.12382), [[ICML2024b]](http://arxiv.org/abs/2402.03683), and applications [[arXiv2025b]](https://arxiv.org/abs/2502.10826)  
  - identifying pitfalls of evidential deep learning [[NeurIPS2024](http://arxiv.org/abs/2402.06160)]

[//]: # (- **Some unifying views on nearest-neighbor methods**)

[//]: # (  - unified view on density functional estimation with fixed-k-NNs [[TIT2022]]&#40;http://arxiv.org/abs/1805.08342&#41;)

[//]: # (  - efficient small-k-nearest-neighbors algorithms [[arXiv2022]]&#40;http://arxiv.org/abs/2202.02464&#41;)
  
[//]: # (- **Information-theoretic tools for machine learning**)

[//]: # (  - from universal compression to parameter-free online optimization [[AISTATS2022]]&#40;http://arxiv.org/abs/2202.02406&#41;)

[//]: # (  - information-theoretic common representation learning &#40;[variational Wyner model [arXiv]]&#40;http://arxiv.org/abs/1905.10945&#41;&#41;)

[//]: # (  - unifying evidential deep learning methods for uncertainty quantification [[NeurIPS2024]&#40;http://arxiv.org/abs/2402.06160&#41;])

As an information theorist by training, I enjoy doing research by simplifying intricate ideas, unifying concepts, and generalizing them to address complex problems.

Check out my [resume](/resume) for more information.
