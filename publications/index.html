<!DOCTYPE html> <html lang=""> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Jongha Jon Ryu </title> <meta name="author" content="Jongha Jon Ryu"> <meta name="description" content="Publications in reverse chronological order.&lt;br&gt; note: * indicates equal contributions. † indicates that the author ordering is alphabetical."> <meta name="keywords" content="Jongha Ryu, J. Jon Ryu, 류종하"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jongharyu.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Jongha <span class="font-weight-bold">Jon</span> Ryu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Publications in reverse chronological order.<br> note: * indicates equal contributions. † indicates that the author ordering is alphabetical.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="pub-filters mb-3"> <span><b>filter by topic</b>:</span> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="operator-learning"> operator learning </label> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="probabilistic-modeling"> probabilistic modeling </label> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="uncertainty-quantification"> uncertainty quantification </label> <br> <span><b>filter by subtopic</b>:</span> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="deep-learning"> deep learning </label> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="information-theory"> information theory </label> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="statistics"> statistics </label> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="sequential-decision-making"> sequential decision making </label> <label class="ml-2"> <input type="checkbox" class="topic-filter" data-topic="divergence-matching"> divergence matching </label> </div> <div class="btn-group btn-group-sm mb-3" role="group" aria-label="Grouping toggle"> <button id="btn-group-year" type="button" class="btn btn-primary active">By year</button> <button id="btn-group-type" type="button" class="btn btn-outline-primary">By venue type</button> </div> <div class="btn-group btn-group-sm mb-3" role="group" aria-label="Selection toggle"> <button id="btn-show-selected" type="button" class="btn btn-primary active"> selected </button> <button id="btn-show-all" type="button" class="btn btn-outline-primary"> all </button> </div> <div id="pubs-by-year"> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,statistics,divergence-matching" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2025b.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2025b.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Shah--Wornell2025" class="col-sm-8"> <div class="title">A Unified View on Learning Unnormalized Distributions via Noise-Contrastive Estimation</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://abhin-shah.github.io/" rel="external nofollow noopener" target="_blank">Abhin Shah</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We provide a unified perspective on various methods for learning unnormalized distributions through the lens of noise-contrastive estimation. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.18209" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/icml2025b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/icml2025-nce-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>This paper studies a family of estimators based on noise-contrastive estimation (NCE) for learning unnormalized distributions. The main contribution of this work is to provide a unified perspective on various methods for learning unnormalized distributions, which have been independently proposed and studied in separate research communities, through the lens of NCE. This unified view offers new insights into existing estimators. Specifically, for exponential families, we establish the finite-sample convergence rates of the proposed estimators under a set of regularity assumptions, most of which are new.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning,information-theory,information-measures,divergence-matching" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2025a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2025a.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jayashankar--Ryu--Wornell2025" class="col-sm-8"> <div class="title">Score-of-Mixture Training: Training One-Step Generative Models Made Simple</div> <div class="author"> <a href="https://tejasjayashankar.github.io/" rel="external nofollow noopener" target="_blank">Tejas Jayashankar<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Spotlight</a> <div class="tldr"> <strong>TL;DR:</strong> We propose a new method for training one-step generative models by minimizing the α-skew Jensen–Shannon divergence using score-based gradient estimates. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.09609" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/icml2025a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/tkj516/score-of-mixture-training" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/icml2025-smt-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Top 2.6% of submissions</p> </div> <div class="abstract hidden"> <p>We propose Score-of-Mixture Training (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the α-skew Jensen-Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call Score-of-Mixture Distillation (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even outperform existing methods.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="uncertainty-quantification,information-theory,sequential-decision-making,statistics" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://learningtheory.org/" rel="external nofollow noopener" target="_blank">COLT</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/colt2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="colt2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Kwon--Koppe--Jun2025" class="col-sm-8"> <div class="title">Improved Offline Contextual Bandits with Second-Order Bounds: Betting and Freezing</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://kwonchungli.github.io/" rel="external nofollow noopener" target="_blank">Jeongyeol Kwon</a>, Benjamin Koppe, and <a href="https://kwangsungjun.github.io/" rel="external nofollow noopener" target="_blank">Kwang-Sung Jun</a> </div> <div class="periodical"> <em>In COLT</em>, June 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We introduce a betting-based confidence bound for off-policy selection, and a new off-policy learning technique that provides a favorable bias-variance tradeoff. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.10826" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/colt2025.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/confidence-sequence-via-gambling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We consider off-policy selection and learning in contextual bandits, where the learner aims to select or train a reward-maximizing policy using data collected by a fixed behavior policy. Our contribution is two-fold. First, we propose a novel off-policy selection method that leverages a new betting-based confidence bound applied to an inverse propensity weight sequence. Our theoretical analysis reveals that this method achieves a significantly improved, variance-adaptive guarantee over prior work. Second, we propose a novel and generic condition on the optimization objective for off-policy learning that strikes a different balance between bias and variance. One special case, which we call freezing, tends to induce low variance, which is preferred in small-data regimes. Our analysis shows that it matches the best existing guarantees. In our empirical study, our selection method outperforms existing methods, and freezing exhibits improved performance in small-sample regimes.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2025a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2025a.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong--Ryu--Yun--Wornell2025" class="col-sm-8"> <div class="title">Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems</div> <div class="author"> <a href="https://www.linkedin.com/in/minchan-jeong-5303b7268/" rel="external nofollow noopener" target="_blank">Minchan Jeong<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, <a href="https://fbsqkd.github.io/" rel="external nofollow noopener" target="_blank">Se-Young Yun</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We show that NeuralSVD (NestedLoRA) provides a scalable and stable approach for learning top-k Koopman singular functions, avoiding the unstable SVD- and inversion-based steps required by VAMPnet/DPNet. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2507.07222" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=kL2pnzClyD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2025a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/MinchanJeong/NeuralKoopmanSVD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The Koopman operator provides a principled framework for analyzing nonlinear dynamical systems through linear operator theory. Recent advances in dynamic mode decomposition (DMD) have shown that trajectory data can be used to identify dominant modes of a system in a data-driven manner. Building on this idea, deep learning methods such as VAMPnet and DPNet have been proposed to learn the leading singular subspaces of the Koopman operator. However, these methods require backpropagation through potentially numerically unstable operations on empirical second moment matrices, such as singular value decomposition and matrix inversion, during objective computation, which can introduce biased gradient estimates and hinder scalability to large systems. In this work, we propose a scalable and conceptually simple method for learning the top-k singular functions of the Koopman operator for stochastic dynamical systems based on the idea of low-rank approximation. Our approach eliminates the need for unstable linear algebraic operations and integrates easily into modern deep learning pipelines. Empirical results demonstrate that the learned singular subspaces are both reliable and effective for downstream tasks such as eigen-analysis and multi-step prediction.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2025b.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2025b.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Zhou--Wornell2025" class="col-sm-8"> <div class="title">Revisiting Orbital Minimization Method for Neural Operator Decomposition</div> <div class="author"> <em>J. Jon Ryu</em>, Samuel Zhou, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We revisit OMM through a modern lens, developing a scalable NestedOMM formulation that trains neural networks to decompose positive semidefinite operators. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.21952" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=AlRJVX5CRi" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2025b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/operator-omm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Spectral decomposition of linear operators plays a central role in many areas of machine learning and scientific computing. Recent work has explored training neural networks to approximate eigenfunctions of such operators, enabling scalable approaches to representation learning, dynamical systems, and partial differential equations (PDEs). In this paper, we revisit a classical optimization framework from the computational physics literature known as the *orbital minimization method* (OMM), originally proposed in the 1990s for solving eigenvalue problems in computational chemistry. We provide a simple linear algebraic proof of the consistency of the OMM objective, and reveal connections between this method and several ideas that have appeared independently across different domains. Our primary goal is to justify its broader applicability in modern learning pipelines. We adapt this framework to train neural networks to decompose positive semidefinite operators, and demonstrate its practical advantages across a range of benchmark tasks. Our results highlight how revisiting classical numerical methods through the lens of modern theory and computation can provide not only a principled approach for deploying neural networks in numerical analysis, but also effective and scalable tools for machine learning.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning,large-language-models,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="Yun--Kim--Park--Kim--Ryu--Cho--Jun2025" class="col-sm-8"> <div class="title">Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model</div> <div class="author"> Jihun Yun, Juno Kim, Jongho Park, Junhyuck Kim, <em>J. Jon Ryu</em>, Jaewoong Cho, and <a href="https://kwangsungjun.github.io/" rel="external nofollow noopener" target="_blank">Kwang-Sung Jun</a> </div> <div class="periodical"> June 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We recast alignment as distribution learning from preferences, deriving three objectives with O(1/n) convergence to the target LM and without the degeneracy issues of RLHF/DPO. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2506.01523" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Alignment via reinforcement learning from human feedback (RLHF) has become the dominant paradigm for controlling the quality of outputs from large language models (LLMs). However, when viewed as ‘loss + regularization,’ the standard RLHF objective lacks theoretical justification and incentivizes degenerate, deterministic solutions, an issue that variants such as Direct Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by framing it as distribution learning from pairwise preference feedback by explicitly modeling how information about the target language model bleeds through the preference data. This explicit modeling leads us to propose three principled learning objectives: preference maximum likelihood estimation, preference distillation, and reverse KL minimization. We theoretically show that all three approaches enjoy strong non-asymptotic O(1/n) convergence to the target language model, naturally avoiding degeneracy and reward overfitting. Finally, we empirically demonstrate that our distribution learning framework, especially preference distillation, consistently outperforms or matches the performances of RLHF and DPO across various tasks and models.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,information-theory,information-measures,divergence-matching" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/infonce.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="infonce.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Yeddanapudi--Xu--Wornell2025" class="col-sm-8"> <div class="title">Contrastive Predictive Coding Done Right for Mutual Information Estimation</div> <div class="author"> <em>J. Jon Ryu</em>, Pavan Yeddanapudi, <a href="https://xiangxiangxu.mit.edu/" rel="external nofollow noopener" target="_blank">Xiangxiang Xu</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> October 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We show that InfoNCE is an inconsistent MI estimator, and introduce a minimal modification that enables consistent density-ratio estimation and accurate MI estimation. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.25983" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>The InfoNCE objective, originally introduced for contrastive representation learning, has become a popular choice for mutual information (MI) estimation, despite its indirect connection to MI. In this paper, we demonstrate why InfoNCE should not be regarded as a valid MI estimator, and we introduce a simple modification, which we refer to as InfoNCE-anchor, for accurate MI estimation. Our modification introduces an auxiliary anchor class, enabling consistent density ratio estimation and yielding a plug-in MI estimator with significantly reduced bias. Beyond this, we generalize our framework using proper scoring rules, which recover InfoNCE-anchor as a special case when the log score is employed. This formulation unifies a broad spectrum of contrastive objectives, including NCE, InfoNCE, and f-divergence variants, under a single principled framework. Empirically, we find that InfoNCE-anchor with the log score achieves the most accurate MI estimates; however, in self-supervised representation learning experiments, we find that the anchor does not improve the downstream task performance. These findings corroborate that contrastive representation learning benefits not from accurate MI estimation per se, but from the learning of structured density ratios.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="sequential-decision-making,uncertainty-quantification,information-theory,statistics" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff6f00"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18" rel="external nofollow noopener" target="_blank">TransIT</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/tit2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tit2024.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Bhatt2024" class="col-sm-8"> <div class="title">On Confidence Sequences for Bounded Random Processes via Universal Gambling Strategies</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt</a> </div> <div class="periodical"> <em>IEEE TransIT</em>, October 2024 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We provide a simple two-horse-race perspective for constructing confidence sequences for bounded random processes, demonstrate new properties of the confidence sequence induced by universal portfolio, and propose its computationally efficient relaxations. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.12382" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/document/10645704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/tit2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/confidence-sequence-via-gambling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper considers the problem of constructing a confidence sequence, which is a sequence of confidence intervals that hold uniformly over time, for estimating the mean of bounded real-valued random processes. This paper revisits the gambling-based approach established in the recent literature from a natural \emphtwo-horse race perspective, and demonstrates new properties of the resulting algorithm induced by Cover (1991)’s universal portfolio. The main result of this paper is a new algorithm based on a mixture of lower bounds, which closely approximates the performance of Cover’s universal portfolio with constant per-round time complexity. A higher-order generalization of a lower bound on a logarithmic function in (Fan et al., 2015), which is developed as a key technique for the proposed algorithm, may be of independent interest.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="uncertainty-quantification,fairness" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISIT</abbr> <figure> <picture> <img src="/assets/img/publication_preview/isit2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="isit2024.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Shah--Shen--Ryu--Das--Sattigeri--Bu--Wornell2023" class="col-sm-8"> <div class="title">Group Fairness with Uncertainty in Sensitive Attributes</div> <div class="author"> <a href="https://abhin-shah.github.io/" rel="external nofollow noopener" target="_blank">Abhin Shah</a>, <a href="https://maohaos2.github.io/Maohao/" rel="external nofollow noopener" target="_blank">Maohao Shen</a>, <em>J. Jon Ryu</em>, Subhro Das, Prasanna Sattigeri, <a href="https://buyuheng.github.io/" rel="external nofollow noopener" target="_blank">Yuheng Bu</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ISIT</em>, July 2024 </div> <div class="periodical"> <a href="https://openreview.net/forum?id=QOe0ekZkCQ" rel="external nofollow noopener" target="_blank">A preliminary version</a> of this manuscript was presented at <a href="https://icml.cc/virtual/2023/workshop/21493" rel="external nofollow noopener" target="_blank">ICML 2023 Workshop on Spurious Correlations, Invariance, and Stability</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a bootstrap-based algorithm that achieves the target level of fairness despite the uncertainty in sensitive attributes. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.08077" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/isit2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Learning a fair predictive model is crucial to mitigate biased decisions against minority groups in high-stakes applications. A common approach to learn such a model involves solving an optimization problem that maximizes the predictive power of the model under an appropriate group fairness constraint. However, in practice, sensitive attributes are often missing or noisy resulting in uncertainty. We demonstrate that solely enforcing fairness constraints on uncertain sensitive attributes can fall significantly short in achieving the level of fairness of models trained without uncertainty. To overcome this limitation, we propose a bootstrap-based algorithm that achieves the target level of fairness despite the uncertainty in sensitive attributes. The algorithm is guided by a Gaussian analysis for the independence notion of fairness where we propose a robust quadratically constrained quadratic problem to ensure a strict fairness guarantee with uncertain sensitive attributes. Our algorithm is applicable to both discrete and continuous sensitive attributes and is effective in real-world classification and regression tasks for various group fairness notions, e.g., independence and separation.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2024a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2024a.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Xu--Erol--Bu--Zheng--Wornell2024" class="col-sm-8"> <div class="title">Operator SVD with Neural Networks via Nested Low-Rank Approximation</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://xiangxiangxu.mit.edu/" rel="external nofollow noopener" target="_blank">Xiangxiang Xu</a>, H. S. Melichan Erol, <a href="https://buyuheng.github.io/" rel="external nofollow noopener" target="_blank">Yuheng Bu</a>, <a href="https://lizhongzheng.mit.edu/" rel="external nofollow noopener" target="_blank">Lizhong Zheng</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2024 </div> <div class="periodical"> <a href="https://ml4physicalsciences.github.io/2023/files/NeurIPS_ML4PS_2023_225.pdf" rel="external nofollow noopener" target="_blank">An extended abstract</a> was presented at <a href="https://ml4physicalsciences.github.io/2023/" rel="external nofollow noopener" target="_blank">Machine Learning and the Physical Sciences Workshop, NeurIPS 2023</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose an efficient unconstrained nested optimization framework for computing the leading singular values and singular functions of a linear operator using neural networks. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.03655" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=qESG5HaaoJ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icml2024a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/neural-svd" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/icml2024-neuralsvd-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/ita2024-neuralsvd.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-L singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cases in computational physics and machine learning.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="sequential-decision-making,uncertainty-quantification,information-theory,statistics" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2024b.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2024b.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Wornell2024" class="col-sm-8"> <div class="title">Gambling-Based Confidence Sequences for Bounded Random Vectors</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Spotlight</a> <div class="tldr"> <strong>TL;DR:</strong> We propose a new approach to constructing confidence sequences for means of bounded multivariate stochastic processes using a general gambling framework. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.03683" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=mu7Er7f9NQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icml2024b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/confidence-sequence-via-gambling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/icml2024-gambling-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Top 3.5% of submissions</p> </div> <div class="abstract hidden"> <p>A confidence sequence (CS) is a sequence of confidence sets that contains a target parameter of an underlying stochastic process at any time step with high probability. This paper proposes a new approach to constructing CSs for means of bounded multivariate stochastic processes using a general gambling framework, extending the recently established coin toss framework for bounded random processes. The proposed gambling framework provides a general recipe for constructing CSs for categorical and probability-vector-valued observations, as well as for general bounded multidimensional observations through a simple reduction. This paper specifically explores the use of the mixture portfolio, akin to Cover’s universal portfolio, in the proposed framework and investigates the properties of the resulting CSs. Simulations demonstrate the tightness of these confidence sequences compared to existing methods. When applied to the sampling without-replacement setting for finite categorical data, it is shown that the resulting CS based on a universal gambling strategy is provably tighter than that of the posterior-prior ratio martingale proposed by Waudby-Smith and Ramdas.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="uncertainty-quantification,deep-learning,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2024.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Shen--Ryu--Ghosh--Bu--Sattigeri--Das--Wornell2024" class="col-sm-8"> <div class="title">Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?</div> <div class="author"> <a href="https://maohaos2.github.io/Maohao/" rel="external nofollow noopener" target="_blank">Maohao Shen<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, Soumya Ghosh, <a href="https://buyuheng.github.io/" rel="external nofollow noopener" target="_blank">Yuheng Bu</a>, Prasanna Sattigeri, Subhro Das, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2024 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We theoretically characterize the pitfalls of evidential deep learning (EDL) in quantifying predictive uncertainty by unifying various EDL objective functions, and empirically demonstrate their failure modes. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.06160" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=P6nVDZRZRB" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/maohaos2/EDL-Mirage" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/neurips2024-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/neurips2024-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>This paper questions the effectiveness of a modern predictive uncertainty quantification approach, called \emphevidential deep learning (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. Despite their perceived strong empirical performance on downstream tasks, a line of recent studies by Bengs et al. identify limitations of the existing methods to conclude their learned epistemic uncertainties are unreliable, e.g., in that they are non-vanishing even with infinite data. Building on and sharpening such analysis, we 1) provide a sharper understanding of the asymptotic behavior of a wide class of EDL methods by unifying various objective functions; 2) reveal that the EDL methods can be better interpreted as an out-of-distribution detection algorithm based on energy-based-models; and 3) conduct extensive ablation studies to better assess their empirical effectiveness with real-world datasets. Through all these analyses, we conclude that even when EDL methods are empirically effective on downstream tasks, this occurs despite their poor uncertainty quantification capabilities. Our investigation suggests that incorporating model uncertainty can help EDL methods faithfully quantify uncertainties and further improve performance on representative downstream tasks, albeit at the cost of additional computational complexity.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <div>Tech. Report</div> </abbr> </div> <div id="Jayashankar--Ryu--Xu--Wornell2024" class="col-sm-8"> <div class="title">Lifted Residual Score Estimation</div> <div class="author"> <a href="https://tejasjayashankar.github.io/" rel="external nofollow noopener" target="_blank">Tejas Jayashankar<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, <a href="https://xiangxiangxu.mit.edu/" rel="external nofollow noopener" target="_blank">Xiangxiang Xu</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> December 2024 </div> <div class="periodical"> <a href="https://openreview.net/forum?id=zX27NDlfcu" rel="external nofollow noopener" target="_blank">An extended abstract</a> was presented at <a href="https://spigmworkshop2024.github.io/" rel="external nofollow noopener" target="_blank">ICML 2024 Workshop on Structured Probabilistic Inference &amp; Generative Modeling</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose lifted score estimation and residual score estimation, two complementary techniques that together improve score learning across VAEs, WAEs, and diffusion models. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper proposes two new techniques to improve the accuracy of score estimation. The first proposal is a new objective function called the \emphlifted score estimation objective, which serves as a replacement for the score matching (SM) objective. Instead of minimizing the expected \ell_2^2-distance between the learned and true score models, the proposed objective operates in the \emphlifted space of the outer-product of a vector-valued function with itself. The distance is defined as the expected squared Frobenius norm of the difference between such matrix-valued objects induced by the learned and true score functions. The second idea is to model and learn the \emphresidual approximation error of the learned score estimator, given a base score model architecture. We empirically demonstrate that the combination of the two ideas called \emphlifted residual score estimation outperforms sliced SM in training VAE and WAE with implicit encoders, and denoising SM in training diffusion models, as evaluated by downstream metrics of sample quality such as the FID score.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="information-theory,sequential-decision-making" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://aistats.org/" rel="external nofollow noopener" target="_blank">AISTATS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/aistats2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aistats2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bhatt--Ryu--Kim2023" class="col-sm-8"> <div class="title">On Universal Portfolios with Continuous Side Information</div> <div class="author"> <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In AISTATS</em>, April 2023 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a universal portfolio strategy that adapts to a continuous side-information sequence. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.02431" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.mlr.press/v206/bhatt23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/aistats2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A new portfolio selection strategy that adapts to a continuous side-information sequence is presented, with a universal wealth guarantee against a class of state-constant rebalanced portfolios with respect to a state function that maps each side-information symbol to a finite set of states. In particular, given that a state function belongs to a collection of functions of finite Natarajan dimension, the proposed strategy is shown to achieve, asymptotically to first order in the exponent, the same wealth as the best state-constant rebalanced portfolio with respect to the best state function, chosen in hindsight from observed market. This result can be viewed as an extension of the seminal work of Cover and Ordentlich (1996) that assumes a single state function.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="information-theory,statistics,information-measures" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff6f00"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18" rel="external nofollow noopener" target="_blank">TransIT</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/tit2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tit2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Ganguly--Kim--Noh--Lee2018" class="col-sm-8"> <div class="title">Nearest neighbor density functional estimation from inverse Laplace transform</div> <div class="author"> <em>J. Jon Ryu<sup>*</sup></em>, <a href="https://acsweb.ucsd.edu/~shgangul/" rel="external nofollow noopener" target="_blank">Shouvik Ganguly<sup>*</sup></a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a>, <a href="http://aais.hanyang.ac.kr/nohyung/" rel="external nofollow noopener" target="_blank">Yung-Kyun Noh</a>, and <a href="https://www.ece.cornell.edu/faculty-directory/daniel-dongyuel-lee" rel="external nofollow noopener" target="_blank">Daniel D. Lee</a> </div> <div class="periodical"> <em>IEEE TransIT</em>, February 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a general recipe to design a L_2-consistent estimator for general density functionals (such as KL divergence) based on k-nearest neighbors. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1805.08342" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/document/9712283" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/tit2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/knn-functional-estimation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/talks/kias2022-knn.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A new approach to L_2-consistent estimation of a general density functional using k-nearest neighbor distances is proposed, where the functional under consideration is in the form of the expectation of some function f of the densities at each point. The estimator is designed to be asymptotically unbiased, using the convergence of the normalized volume of a k-nearest neighbor ball to a Gamma distribution in the large-sample limit, and naturally involves the inverse Laplace transform of a scaled version of the function f. Some instantiations of the proposed estimator recover existing k-nearest neighbor based estimators of Shannon and Rényi entropies and Kullback-Leibler and Rényi divergences, and discover new consistent estimators for many other functionals such as logarithmic entropies and divergences. The L_2-consistency of the proposed estimator is established for a broad class of densities for general functionals, and the convergence rate in mean squared error is established as a function of the sample size for smooth, bounded densities.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="information-theory,sequential-decision-making" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://aistats.org/" rel="external nofollow noopener" target="_blank">AISTATS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/aistats2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aistats2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Bhatt--Kim2022" class="col-sm-8"> <div class="title">Parameter-Free Online Linear Optimization with Side Information via Universal Coin Betting</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt</a>, and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In AISTATS</em>, March 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a parameter-free online linear optimization algorithm that adapts to **tree-structured side information**. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.02406" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.mlr.press/v151/ryu22a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/aistats2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/olo-with-side-information" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/aistats2022-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/aistats2022-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A class of parameter-free online linear optimization algorithms is proposed that harnesses the structure of an adversarial sequence by adapting to some side information. These algorithms combine the reduction technique of Orabona and Pál (2016) for adapting coin betting algorithms for online linear optimization with universal compression techniques in information theory for incorporating sequential side information to coin betting. Concrete examples are studied in which the side information has a tree structure and consists of quantized values of the previous symbols of the adversarial sequence, including fixed-order and variable-order Markov cases. By modifying the context-tree weighting technique of Willems, Shtarkov, and Tjalkens (1995), the proposed algorithm is further refined to achieve the best performance over all adaptive algorithms with tree-structured side information of a given maximum order in a computationally efficient manner.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">PhD thesis</abbr> </div> <div id="Ryu2022" class="col-sm-8"> <div class="title">From Information Theory to Machine Learning Algorithms: A Few Vignettes</div> <div class="author"> <em>Jongha Jon Ryu</em> </div> <div class="periodical"> <em>University of California San Diego</em>, September 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://escholarship.org/uc/item/5fc8x66w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This dissertation illustrates how certain information-theoretic ideas and views on learning problems can lead to new algorithms via concrete examples.The three information-theoretic strategies taken in this dissertation are (1) to abstract out the gist of a learning problem in the infinite-sample limit; (2) to reduce a learning problem into a probability estimation problem and plugging-in a "good" probability; and (3) to adapt and apply relevant results from information theory. These are applied to three topics in machine learning, including representation learning, nearest-neighbor methods, and universal information processing, where two problems are studied from each topic.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="information-theory,statistics" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/fixedknn.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fixedknn.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Kim2021" class="col-sm-8"> <div class="title">Minimax Optimal Algorithms with Fixed-k-Nearest Neighbors</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> September 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We present how to perform minimax optimal classification, regression, and density estimation based on small-k nearest neighbor (NN) searches. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.02464v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/jongharyu/split-knn-rules" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper presents how to perform minimax optimal classification, regression, and density estimation based on fixed-k nearest neighbor (NN) searches. We consider a distributed learning scenario, in which a massive dataset is split into smaller groups, where the k-NNs are found for a query point with respect to each subset of data. We propose \emphoptimal rules to aggregate the fixed-k-NN information for classification, regression, and density estimation that achieve minimax optimal rates for the respective problems. We show that the distributed algorithm with a fixed k over a sufficiently large number of groups attains a minimax optimal error rate up to a multiplicative logarithmic factor under some regularity conditions. Roughly speaking, distributed k-NN rules with M groups has a performance comparable to the standard Θ(kM)-NN rules even for fixed k.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,information-theory,deep-learning,representation-learning,information-measures,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/wyner.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="wyner.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Choi--Kim--El-Khamy--Lee2022" class="col-sm-8"> <div class="title">Learning with Succinct Common Representation with Wyner’s Common Information</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://scholar.google.com/citations?user=haggDAwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yoojin Choi</a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a>, <a href="https://scholar.google.com/citations?user=qxPC268AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Mostafa El-Khamy</a>, and <a href="https://scholar.google.com/citations?user=Cya7Va8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jungwon Lee</a> </div> <div class="periodical"> September 2022 </div> <div class="periodical"> Preliminary versions were presented at <a href="http://bayesiandeeplearning.org/2018/papers/122.pdf" rel="external nofollow noopener" target="_blank">the Bayesian Deep Learning Workshop at NeurIPS 2018</a> and <a href="http://bayesiandeeplearning.org/2021/papers/67.pdf" rel="external nofollow noopener" target="_blank">at the Bayesian Deep Learning workshop at NeurIPS 2021</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a bimodal generative model that operationalizes Wyner’s common information by learning a succinct shared representation for joint and conditional generation. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1905.10945" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/jongharyu/wyner-model" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/talk/wyner2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A new bimodal generative model is proposed for generating conditional and joint samples, accompanied with a training method with learning a succinct bottleneck representation. The proposed model, dubbed as the variational Wyner model, is designed based on two classical problems in network information theory—distributed simulation and channel synthesis—in which Wyner’s common information arises as the fundamental limit on the succinctness of the common representation. The model is trained by minimizing the symmetric Kullback–Leibler divergence between variational and model distributions with regularization terms for common information, reconstruction consistency, and latent space matching terms, which is carried out via an adversarial density ratio estimation technique. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with synthetic and real-world datasets, as well as a challenging zero-shot image retrieval task.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="information-theory,statistics" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="Ryu--Kim2022" class="col-sm-8"> <div class="title">An Information-Theoretic Proof of Kac–Bernstein Theorem</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> September 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We give a short information-theoretic proof of the Kac–Bernstein characterization of the normal distribution. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.06005" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>A short, information-theoretic proof of the Kac–Bernstein theorem, which is stated as follows, is presented: For any independent random variables X and Y, if X+Y and X-Y are independent, then X and Y are normally distributed.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="operator-learning,representation-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISIT</abbr> <figure> <picture> <img src="/assets/img/publication_preview/isit2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="isit2021.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Huang--Kim2021" class="col-sm-8"> <div class="title">On the Role of Eigendecomposition in Kernel Embedding</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://scholar.google.com/citations?user=nRIpwSwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jiun-Ting Huang</a>, and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In ISIT</em>, September 2021 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose an eigendecomposition-free kernel embedding method that only requires density estimation at query points. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9517746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/isit2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/talks/isit2021-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>This paper proposes a special variant of Laplacian eigenmaps, whose solution is characterized by the underlying density and the eigenfunctions of the associated Hilbert–Schmidt operator of a similarity kernel function. In contrast to existing kernel-based spectral methods such as kernel principal component analysis and Laplacian eigenmaps, the new embedding algorithm only involves estimating density at each query point without any eigendecomposition of a matrix. A concrete example of dot-product kernels over hypersphere is provided to illustrate the applicability of the proposed framework.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="deep-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICASSP</abbr> <figure> <picture> <img src="/assets/img/publication_preview/icassp2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icassp2020.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yang--Sautiere--Ryu--Cohen2020" class="col-sm-8"> <div class="title">Feedback Recurrent Autoencoder</div> <div class="author"> <a href="https://yyang768osu.github.io/" rel="external nofollow noopener" target="_blank">Yang Yang</a>, Guillaume Sautière, <em>J. Jon Ryu</em>, and <a href="https://tacocohen.wordpress.com/" rel="external nofollow noopener" target="_blank">Taco S. Cohen</a> </div> <div class="periodical"> <em>In Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</em>, September 2020 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a new recurrent autoencoder architecture for online compression of sequential data with temporal dependency. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1911.04018" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/document/9054074" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icassp2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this work, we propose a new recurrent autoencoder architecture, termed Feedback Recurrent AutoEncoder (FRAE), for online compression of sequential data with temporal dependency. The recurrent structure of FRAE is designed to efficiently extract the redundancy along the time dimension and allows a compact discrete representation of the data to be learned. We demonstrate its effectiveness in speech spectrogram compression. Specifically, we show that the FRAE, paired with a powerful neural vocoder, can produce high-quality speech waveforms at a low, fixed bitrate. We further show that by adding a learned prior for the latent space and using an entropy coder, we can achieve an even lower variable bitrate.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="information-theory,deep-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICIP</abbr> <figure> <picture> <img src="/assets/img/publication_preview/icip2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icip2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Kim2018" class="col-sm-8"> <div class="title">Conditional distribution learning with neural networks and its application to universal image denoising</div> <div class="author"> <em>Jongha Ryu</em> and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In Proc. IEEE Int. Conf. Image Proc. (ICIP)</em>, September 2018 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We extend a simple deep-learning-extension of the information-theoretic universal denoising algorithm DUDE for structured, large-alphabet problems such as image denoising. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8451573/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icip2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/icip2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/icip2018-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A simple and scalable denoising algorithm is proposed that can be applied to a wide range of source and noise models. At the core of the proposed CUDE algorithm is symbol-by-symbol universal denoising used by the celebrated DUDE algorithm, whereby the optimal estimate of the source from an unknown distribution is computed by inverting the empirical distribution of the noisy observation sequence by a deep neural network, which naturally and implicitly aggregates multi-pie contexts of similar characteristics and estimates the conditional distribution more accurately. The performance of CUDE is evaluated for grayscale images of varying bit depths, which improves upon DUDE and its recent neural network based extension, Neural DUDE.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="information-theory,statistics" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ITW</abbr> <figure> <picture> <img src="/assets/img/publication_preview/itw2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="itw2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bhatt--Huang--Kim--Ryu--Sen2018ITW" class="col-sm-8"> <div class="title">Variations on a theme by Liu, Cuff, and Verdú: The power of posterior sampling</div> <div class="author"> <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt<sup>†</sup></a>, <a href="https://scholar.google.com/citations?user=nRIpwSwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jiun-Ting Huang<sup>†</sup></a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim<sup>†</sup></a>, <em>J. Jon Ryu<sup>†</sup></em>, and Pinar Sen<sup>†</sup> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="alphabetically ordered"> </i> </div> <div class="periodical"> <em>In ITW</em>, September 2018 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We explore the power of multiple random guesses in statistical inference. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8613436" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/itw2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/talks/itw2018-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>The Liu–Cuff–Verdu lemma states that in estimating a source X from an observation Y, making a random guess X’ from the posterior p(x|y) can go wrong at most twice as often as the optimal answer. Several variations of this fundamental, yet rather arcane, result are explored for detection, decoding, and estimation problems.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="information-theory" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Allerton</abbr> <figure> <picture> <img src="/assets/img/publication_preview/allerton2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="allerton2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bhatt--Huang--Kim--Ryu--Sen2018Allerton" class="col-sm-8"> <div class="title">Monte Carlo methods for randomized likelihood decoding</div> <div class="author"> <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt<sup>†</sup></a>, <a href="https://scholar.google.com/citations?user=nRIpwSwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jiun-Ting Huang<sup>†</sup></a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim<sup>†</sup></a>, <em>J. Jon Ryu<sup>†</sup></em>, and Pinar Sen<sup>†</sup> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="alphabetically ordered"> </i> </div> <div class="periodical"> <em>In Allerton</em>, September 2018 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We explore the potential of randomized decoding based on sampling from the posterior distribution via Monte Carlo techniques. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8636049/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/allerton2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A randomized decoder that generates the message estimate according to the posterior distribution is known to achieve the reliability comparable to that of the maximum a posteriori probability decoder. With a goal of practical implementations of such a randomized decoder, several Monte Carlo techniques, such as rejection sampling, Gibbs sampling, and the Metropolis algorithm, are adapted to the problem of efficient sampling from the posterior distribution. Analytical and experimental results compare the complexity and performance of these Monte Carlo decoders for simple linear codes and the binary symmetric channel.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="Yoo--Ha--Yi--Ryu--Kim--Ha--Kim--Yoon2017" class="col-sm-8"> <div class="title">Energy-based sequence GANs for recommendation and their connection to imitation learning</div> <div class="author"> Jaeyoon Yoo, Heonseok Ha, Jihun Yi, <em>Jongha Ryu</em>, Chanju Kim, Jung-Woo Ha, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a>, and Sungroh Yoon </div> <div class="periodical"> September 2017 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We reinterpret EB-SeqGANs for recommendation as maximum-entropy imitation learning by viewing the energy function as a feature function. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1706.09200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Recommender systems aim to find an accurate and efficient mapping from historic data of user-preferred items to a new item that is to be liked by a user. Towards this goal, energy-based sequence generative adversarial nets (EB-SeqGANs) are adopted for recommendation by learning a generative model for the time series of user-preferred items. By recasting the energy function as the feature function, the proposed EB-SeqGANs is interpreted as an instance of maximum-entropy imitation learning.</p> </div> </div> </div> </li></ol> </div> </div> <div id="pubs-by-type" style="display:none;"> <div class="publications"> <h2 class="bibliography">Preprints</h2> <h3 class="bibliography">2025</h3> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning,large-language-models,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="Yun--Kim--Park--Kim--Ryu--Cho--Jun2025" class="col-sm-8"> <div class="title">Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model</div> <div class="author"> Jihun Yun, Juno Kim, Jongho Park, Junhyuck Kim, <em>J. Jon Ryu</em>, Jaewoong Cho, and <a href="https://kwangsungjun.github.io/" rel="external nofollow noopener" target="_blank">Kwang-Sung Jun</a> </div> <div class="periodical"> June 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We recast alignment as distribution learning from preferences, deriving three objectives with O(1/n) convergence to the target LM and without the degeneracy issues of RLHF/DPO. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2506.01523" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Alignment via reinforcement learning from human feedback (RLHF) has become the dominant paradigm for controlling the quality of outputs from large language models (LLMs). However, when viewed as ‘loss + regularization,’ the standard RLHF objective lacks theoretical justification and incentivizes degenerate, deterministic solutions, an issue that variants such as Direct Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by framing it as distribution learning from pairwise preference feedback by explicitly modeling how information about the target language model bleeds through the preference data. This explicit modeling leads us to propose three principled learning objectives: preference maximum likelihood estimation, preference distillation, and reverse KL minimization. We theoretically show that all three approaches enjoy strong non-asymptotic O(1/n) convergence to the target language model, naturally avoiding degeneracy and reward overfitting. Finally, we empirically demonstrate that our distribution learning framework, especially preference distillation, consistently outperforms or matches the performances of RLHF and DPO across various tasks and models.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,information-theory,information-measures,divergence-matching" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/infonce.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="infonce.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Yeddanapudi--Xu--Wornell2025" class="col-sm-8"> <div class="title">Contrastive Predictive Coding Done Right for Mutual Information Estimation</div> <div class="author"> <em>J. Jon Ryu</em>, Pavan Yeddanapudi, <a href="https://xiangxiangxu.mit.edu/" rel="external nofollow noopener" target="_blank">Xiangxiang Xu</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> October 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We show that InfoNCE is an inconsistent MI estimator, and introduce a minimal modification that enables consistent density-ratio estimation and accurate MI estimation. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.25983" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>The InfoNCE objective, originally introduced for contrastive representation learning, has become a popular choice for mutual information (MI) estimation, despite its indirect connection to MI. In this paper, we demonstrate why InfoNCE should not be regarded as a valid MI estimator, and we introduce a simple modification, which we refer to as InfoNCE-anchor, for accurate MI estimation. Our modification introduces an auxiliary anchor class, enabling consistent density ratio estimation and yielding a plug-in MI estimator with significantly reduced bias. Beyond this, we generalize our framework using proper scoring rules, which recover InfoNCE-anchor as a special case when the log score is employed. This formulation unifies a broad spectrum of contrastive objectives, including NCE, InfoNCE, and f-divergence variants, under a single principled framework. Empirically, we find that InfoNCE-anchor with the log score achieves the most accurate MI estimates; however, in self-supervised representation learning experiments, we find that the anchor does not improve the downstream task performance. These findings corroborate that contrastive representation learning benefits not from accurate MI estimation per se, but from the learning of structured density ratios.</p> </div> </div> </div> </li> </ol> <h3 class="bibliography">2022</h3> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="information-theory,statistics" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/fixedknn.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fixedknn.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Kim2021" class="col-sm-8"> <div class="title">Minimax Optimal Algorithms with Fixed-k-Nearest Neighbors</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> October 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We present how to perform minimax optimal classification, regression, and density estimation based on small-k nearest neighbor (NN) searches. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.02464v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/jongharyu/split-knn-rules" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper presents how to perform minimax optimal classification, regression, and density estimation based on fixed-k nearest neighbor (NN) searches. We consider a distributed learning scenario, in which a massive dataset is split into smaller groups, where the k-NNs are found for a query point with respect to each subset of data. We propose \emphoptimal rules to aggregate the fixed-k-NN information for classification, regression, and density estimation that achieve minimax optimal rates for the respective problems. We show that the distributed algorithm with a fixed k over a sufficiently large number of groups attains a minimax optimal error rate up to a multiplicative logarithmic factor under some regularity conditions. Roughly speaking, distributed k-NN rules with M groups has a performance comparable to the standard Θ(kM)-NN rules even for fixed k.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,information-theory,deep-learning,representation-learning,information-measures,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/wyner.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="wyner.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Choi--Kim--El-Khamy--Lee2022" class="col-sm-8"> <div class="title">Learning with Succinct Common Representation with Wyner’s Common Information</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://scholar.google.com/citations?user=haggDAwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Yoojin Choi</a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a>, <a href="https://scholar.google.com/citations?user=qxPC268AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Mostafa El-Khamy</a>, and <a href="https://scholar.google.com/citations?user=Cya7Va8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jungwon Lee</a> </div> <div class="periodical"> October 2022 </div> <div class="periodical"> Preliminary versions were presented at <a href="http://bayesiandeeplearning.org/2018/papers/122.pdf" rel="external nofollow noopener" target="_blank">the Bayesian Deep Learning Workshop at NeurIPS 2018</a> and <a href="http://bayesiandeeplearning.org/2021/papers/67.pdf" rel="external nofollow noopener" target="_blank">at the Bayesian Deep Learning workshop at NeurIPS 2021</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a bimodal generative model that operationalizes Wyner’s common information by learning a succinct shared representation for joint and conditional generation. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1905.10945" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/jongharyu/wyner-model" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/talk/wyner2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A new bimodal generative model is proposed for generating conditional and joint samples, accompanied with a training method with learning a succinct bottleneck representation. The proposed model, dubbed as the variational Wyner model, is designed based on two classical problems in network information theory—distributed simulation and channel synthesis—in which Wyner’s common information arises as the fundamental limit on the succinctness of the common representation. The model is trained by minimizing the symmetric Kullback–Leibler divergence between variational and model distributions with regularization terms for common information, reconstruction consistency, and latent space matching terms, which is carried out via an adversarial density ratio estimation technique. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with synthetic and real-world datasets, as well as a challenging zero-shot image retrieval task.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">Journal articles</h2> <h3 class="bibliography">2024</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="sequential-decision-making,uncertainty-quantification,information-theory,statistics" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff6f00"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18" rel="external nofollow noopener" target="_blank">TransIT</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/tit2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tit2024.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Bhatt2024" class="col-sm-8"> <div class="title">On Confidence Sequences for Bounded Random Processes via Universal Gambling Strategies</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt</a> </div> <div class="periodical"> <em>IEEE TransIT</em>, October 2024 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We provide a simple two-horse-race perspective for constructing confidence sequences for bounded random processes, demonstrate new properties of the confidence sequence induced by universal portfolio, and propose its computationally efficient relaxations. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.12382" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/document/10645704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/tit2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/confidence-sequence-via-gambling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper considers the problem of constructing a confidence sequence, which is a sequence of confidence intervals that hold uniformly over time, for estimating the mean of bounded real-valued random processes. This paper revisits the gambling-based approach established in the recent literature from a natural \emphtwo-horse race perspective, and demonstrates new properties of the resulting algorithm induced by Cover (1991)’s universal portfolio. The main result of this paper is a new algorithm based on a mixture of lower bounds, which closely approximates the performance of Cover’s universal portfolio with constant per-round time complexity. A higher-order generalization of a lower bound on a logarithmic function in (Fan et al., 2015), which is developed as a key technique for the proposed algorithm, may be of independent interest.</p> </div> </div> </div> </li></ol> <h3 class="bibliography">2022</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="information-theory,statistics,information-measures" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff6f00"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=18" rel="external nofollow noopener" target="_blank">TransIT</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/tit2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tit2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Ganguly--Kim--Noh--Lee2018" class="col-sm-8"> <div class="title">Nearest neighbor density functional estimation from inverse Laplace transform</div> <div class="author"> <em>J. Jon Ryu<sup>*</sup></em>, <a href="https://acsweb.ucsd.edu/~shgangul/" rel="external nofollow noopener" target="_blank">Shouvik Ganguly<sup>*</sup></a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a>, <a href="http://aais.hanyang.ac.kr/nohyung/" rel="external nofollow noopener" target="_blank">Yung-Kyun Noh</a>, and <a href="https://www.ece.cornell.edu/faculty-directory/daniel-dongyuel-lee" rel="external nofollow noopener" target="_blank">Daniel D. Lee</a> </div> <div class="periodical"> <em>IEEE TransIT</em>, February 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a general recipe to design a L_2-consistent estimator for general density functionals (such as KL divergence) based on k-nearest neighbors. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1805.08342" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/document/9712283" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/tit2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/knn-functional-estimation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/talks/kias2022-knn.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A new approach to L_2-consistent estimation of a general density functional using k-nearest neighbor distances is proposed, where the functional under consideration is in the form of the expectation of some function f of the densities at each point. The estimator is designed to be asymptotically unbiased, using the convergence of the normalized volume of a k-nearest neighbor ball to a Gamma distribution in the large-sample limit, and naturally involves the inverse Laplace transform of a scaled version of the function f. Some instantiations of the proposed estimator recover existing k-nearest neighbor based estimators of Shannon and Rényi entropies and Kullback-Leibler and Rényi divergences, and discover new consistent estimators for many other functionals such as logarithmic entropies and divergences. The L_2-consistency of the proposed estimator is established for a broad class of densities for general functionals, and the convergence rate in mean squared error is established as a function of the sample size for smooth, bounded densities.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">Conference papers</h2> <h3 class="bibliography">2025</h3> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,statistics,divergence-matching" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2025b.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2025b.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Shah--Wornell2025" class="col-sm-8"> <div class="title">A Unified View on Learning Unnormalized Distributions via Noise-Contrastive Estimation</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://abhin-shah.github.io/" rel="external nofollow noopener" target="_blank">Abhin Shah</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We provide a unified perspective on various methods for learning unnormalized distributions through the lens of noise-contrastive estimation. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.18209" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/icml2025b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/icml2025-nce-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>This paper studies a family of estimators based on noise-contrastive estimation (NCE) for learning unnormalized distributions. The main contribution of this work is to provide a unified perspective on various methods for learning unnormalized distributions, which have been independently proposed and studied in separate research communities, through the lens of NCE. This unified view offers new insights into existing estimators. Specifically, for exponential families, we establish the finite-sample convergence rates of the proposed estimators under a set of regularity assumptions, most of which are new.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning,information-theory,information-measures,divergence-matching" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2025a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2025a.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jayashankar--Ryu--Wornell2025" class="col-sm-8"> <div class="title">Score-of-Mixture Training: Training One-Step Generative Models Made Simple</div> <div class="author"> <a href="https://tejasjayashankar.github.io/" rel="external nofollow noopener" target="_blank">Tejas Jayashankar<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Spotlight</a> <div class="tldr"> <strong>TL;DR:</strong> We propose a new method for training one-step generative models by minimizing the α-skew Jensen–Shannon divergence using score-based gradient estimates. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.09609" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/icml2025a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/tkj516/score-of-mixture-training" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/icml2025-smt-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Top 2.6% of submissions</p> </div> <div class="abstract hidden"> <p>We propose Score-of-Mixture Training (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the α-skew Jensen-Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call Score-of-Mixture Distillation (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even outperform existing methods.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="uncertainty-quantification,information-theory,sequential-decision-making,statistics" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://learningtheory.org/" rel="external nofollow noopener" target="_blank">COLT</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/colt2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="colt2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Kwon--Koppe--Jun2025" class="col-sm-8"> <div class="title">Improved Offline Contextual Bandits with Second-Order Bounds: Betting and Freezing</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://kwonchungli.github.io/" rel="external nofollow noopener" target="_blank">Jeongyeol Kwon</a>, Benjamin Koppe, and <a href="https://kwangsungjun.github.io/" rel="external nofollow noopener" target="_blank">Kwang-Sung Jun</a> </div> <div class="periodical"> <em>In COLT</em>, June 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We introduce a betting-based confidence bound for off-policy selection, and a new off-policy learning technique that provides a favorable bias-variance tradeoff. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.10826" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/colt2025.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/confidence-sequence-via-gambling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We consider off-policy selection and learning in contextual bandits, where the learner aims to select or train a reward-maximizing policy using data collected by a fixed behavior policy. Our contribution is two-fold. First, we propose a novel off-policy selection method that leverages a new betting-based confidence bound applied to an inverse propensity weight sequence. Our theoretical analysis reveals that this method achieves a significantly improved, variance-adaptive guarantee over prior work. Second, we propose a novel and generic condition on the optimization objective for off-policy learning that strikes a different balance between bias and variance. One special case, which we call freezing, tends to induce low variance, which is preferred in small-data regimes. Our analysis shows that it matches the best existing guarantees. In our empirical study, our selection method outperforms existing methods, and freezing exhibits improved performance in small-sample regimes.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2025a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2025a.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong--Ryu--Yun--Wornell2025" class="col-sm-8"> <div class="title">Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems</div> <div class="author"> <a href="https://www.linkedin.com/in/minchan-jeong-5303b7268/" rel="external nofollow noopener" target="_blank">Minchan Jeong<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, <a href="https://fbsqkd.github.io/" rel="external nofollow noopener" target="_blank">Se-Young Yun</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We show that NeuralSVD (NestedLoRA) provides a scalable and stable approach for learning top-k Koopman singular functions, avoiding the unstable SVD- and inversion-based steps required by VAMPnet/DPNet. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2507.07222" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=kL2pnzClyD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2025a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/MinchanJeong/NeuralKoopmanSVD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The Koopman operator provides a principled framework for analyzing nonlinear dynamical systems through linear operator theory. Recent advances in dynamic mode decomposition (DMD) have shown that trajectory data can be used to identify dominant modes of a system in a data-driven manner. Building on this idea, deep learning methods such as VAMPnet and DPNet have been proposed to learn the leading singular subspaces of the Koopman operator. However, these methods require backpropagation through potentially numerically unstable operations on empirical second moment matrices, such as singular value decomposition and matrix inversion, during objective computation, which can introduce biased gradient estimates and hinder scalability to large systems. In this work, we propose a scalable and conceptually simple method for learning the top-k singular functions of the Koopman operator for stochastic dynamical systems based on the idea of low-rank approximation. Our approach eliminates the need for unstable linear algebraic operations and integrates easily into modern deep learning pipelines. Empirical results demonstrate that the learned singular subspaces are both reliable and effective for downstream tasks such as eigen-analysis and multi-step prediction.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2025b.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2025b.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Zhou--Wornell2025" class="col-sm-8"> <div class="title">Revisiting Orbital Minimization Method for Neural Operator Decomposition</div> <div class="author"> <em>J. Jon Ryu</em>, Samuel Zhou, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We revisit OMM through a modern lens, developing a scalable NestedOMM formulation that trains neural networks to decompose positive semidefinite operators. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.21952" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=AlRJVX5CRi" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2025b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/operator-omm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Spectral decomposition of linear operators plays a central role in many areas of machine learning and scientific computing. Recent work has explored training neural networks to approximate eigenfunctions of such operators, enabling scalable approaches to representation learning, dynamical systems, and partial differential equations (PDEs). In this paper, we revisit a classical optimization framework from the computational physics literature known as the *orbital minimization method* (OMM), originally proposed in the 1990s for solving eigenvalue problems in computational chemistry. We provide a simple linear algebraic proof of the consistency of the OMM objective, and reveal connections between this method and several ideas that have appeared independently across different domains. Our primary goal is to justify its broader applicability in modern learning pipelines. We adapt this framework to train neural networks to decompose positive semidefinite operators, and demonstrate its practical advantages across a range of benchmark tasks. Our results highlight how revisiting classical numerical methods through the lens of modern theory and computation can provide not only a principled approach for deploying neural networks in numerical analysis, but also effective and scalable tools for machine learning.</p> </div> </div> </div> </li> </ol> <h3 class="bibliography">2024</h3> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="uncertainty-quantification,fairness" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISIT</abbr> <figure> <picture> <img src="/assets/img/publication_preview/isit2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="isit2024.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Shah--Shen--Ryu--Das--Sattigeri--Bu--Wornell2023" class="col-sm-8"> <div class="title">Group Fairness with Uncertainty in Sensitive Attributes</div> <div class="author"> <a href="https://abhin-shah.github.io/" rel="external nofollow noopener" target="_blank">Abhin Shah</a>, <a href="https://maohaos2.github.io/Maohao/" rel="external nofollow noopener" target="_blank">Maohao Shen</a>, <em>J. Jon Ryu</em>, Subhro Das, Prasanna Sattigeri, <a href="https://buyuheng.github.io/" rel="external nofollow noopener" target="_blank">Yuheng Bu</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ISIT</em>, July 2024 </div> <div class="periodical"> <a href="https://openreview.net/forum?id=QOe0ekZkCQ" rel="external nofollow noopener" target="_blank">A preliminary version</a> of this manuscript was presented at <a href="https://icml.cc/virtual/2023/workshop/21493" rel="external nofollow noopener" target="_blank">ICML 2023 Workshop on Spurious Correlations, Invariance, and Stability</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a bootstrap-based algorithm that achieves the target level of fairness despite the uncertainty in sensitive attributes. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.08077" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/isit2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Learning a fair predictive model is crucial to mitigate biased decisions against minority groups in high-stakes applications. A common approach to learn such a model involves solving an optimization problem that maximizes the predictive power of the model under an appropriate group fairness constraint. However, in practice, sensitive attributes are often missing or noisy resulting in uncertainty. We demonstrate that solely enforcing fairness constraints on uncertain sensitive attributes can fall significantly short in achieving the level of fairness of models trained without uncertainty. To overcome this limitation, we propose a bootstrap-based algorithm that achieves the target level of fairness despite the uncertainty in sensitive attributes. The algorithm is guided by a Gaussian analysis for the independence notion of fairness where we propose a robust quadratically constrained quadratic problem to ensure a strict fairness guarantee with uncertain sensitive attributes. Our algorithm is applicable to both discrete and continuous sensitive attributes and is effective in real-world classification and regression tasks for various group fairness notions, e.g., independence and separation.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2024a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2024a.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Xu--Erol--Bu--Zheng--Wornell2024" class="col-sm-8"> <div class="title">Operator SVD with Neural Networks via Nested Low-Rank Approximation</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://xiangxiangxu.mit.edu/" rel="external nofollow noopener" target="_blank">Xiangxiang Xu</a>, H. S. Melichan Erol, <a href="https://buyuheng.github.io/" rel="external nofollow noopener" target="_blank">Yuheng Bu</a>, <a href="https://lizhongzheng.mit.edu/" rel="external nofollow noopener" target="_blank">Lizhong Zheng</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2024 </div> <div class="periodical"> <a href="https://ml4physicalsciences.github.io/2023/files/NeurIPS_ML4PS_2023_225.pdf" rel="external nofollow noopener" target="_blank">An extended abstract</a> was presented at <a href="https://ml4physicalsciences.github.io/2023/" rel="external nofollow noopener" target="_blank">Machine Learning and the Physical Sciences Workshop, NeurIPS 2023</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose an efficient unconstrained nested optimization framework for computing the leading singular values and singular functions of a linear operator using neural networks. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.03655" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=qESG5HaaoJ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icml2024a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/neural-svd" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/icml2024-neuralsvd-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/ita2024-neuralsvd.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-L singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cases in computational physics and machine learning.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="sequential-decision-making,uncertainty-quantification,information-theory,statistics" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2024b.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2024b.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Wornell2024" class="col-sm-8"> <div class="title">Gambling-Based Confidence Sequences for Bounded Random Vectors</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Spotlight</a> <div class="tldr"> <strong>TL;DR:</strong> We propose a new approach to constructing confidence sequences for means of bounded multivariate stochastic processes using a general gambling framework. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.03683" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=mu7Er7f9NQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icml2024b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/confidence-sequence-via-gambling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/icml2024-gambling-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Top 3.5% of submissions</p> </div> <div class="abstract hidden"> <p>A confidence sequence (CS) is a sequence of confidence sets that contains a target parameter of an underlying stochastic process at any time step with high probability. This paper proposes a new approach to constructing CSs for means of bounded multivariate stochastic processes using a general gambling framework, extending the recently established coin toss framework for bounded random processes. The proposed gambling framework provides a general recipe for constructing CSs for categorical and probability-vector-valued observations, as well as for general bounded multidimensional observations through a simple reduction. This paper specifically explores the use of the mixture portfolio, akin to Cover’s universal portfolio, in the proposed framework and investigates the properties of the resulting CSs. Simulations demonstrate the tightness of these confidence sequences compared to existing methods. When applied to the sampling without-replacement setting for finite categorical data, it is shown that the resulting CS based on a universal gambling strategy is provably tighter than that of the posterior-prior ratio martingale proposed by Waudby-Smith and Ramdas.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="uncertainty-quantification,deep-learning,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2024.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Shen--Ryu--Ghosh--Bu--Sattigeri--Das--Wornell2024" class="col-sm-8"> <div class="title">Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?</div> <div class="author"> <a href="https://maohaos2.github.io/Maohao/" rel="external nofollow noopener" target="_blank">Maohao Shen<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, Soumya Ghosh, <a href="https://buyuheng.github.io/" rel="external nofollow noopener" target="_blank">Yuheng Bu</a>, Prasanna Sattigeri, Subhro Das, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2024 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We theoretically characterize the pitfalls of evidential deep learning (EDL) in quantifying predictive uncertainty by unifying various EDL objective functions, and empirically demonstrate their failure modes. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.06160" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=P6nVDZRZRB" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/maohaos2/EDL-Mirage" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/neurips2024-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/neurips2024-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>This paper questions the effectiveness of a modern predictive uncertainty quantification approach, called \emphevidential deep learning (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. Despite their perceived strong empirical performance on downstream tasks, a line of recent studies by Bengs et al. identify limitations of the existing methods to conclude their learned epistemic uncertainties are unreliable, e.g., in that they are non-vanishing even with infinite data. Building on and sharpening such analysis, we 1) provide a sharper understanding of the asymptotic behavior of a wide class of EDL methods by unifying various objective functions; 2) reveal that the EDL methods can be better interpreted as an out-of-distribution detection algorithm based on energy-based-models; and 3) conduct extensive ablation studies to better assess their empirical effectiveness with real-world datasets. Through all these analyses, we conclude that even when EDL methods are empirically effective on downstream tasks, this occurs despite their poor uncertainty quantification capabilities. Our investigation suggests that incorporating model uncertainty can help EDL methods faithfully quantify uncertainties and further improve performance on representative downstream tasks, albeit at the cost of additional computational complexity.</p> </div> </div> </div> </li> </ol> <h3 class="bibliography">2023</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="information-theory,sequential-decision-making" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://aistats.org/" rel="external nofollow noopener" target="_blank">AISTATS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/aistats2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aistats2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bhatt--Ryu--Kim2023" class="col-sm-8"> <div class="title">On Universal Portfolios with Continuous Side Information</div> <div class="author"> <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In AISTATS</em>, April 2023 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a universal portfolio strategy that adapts to a continuous side-information sequence. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.02431" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.mlr.press/v206/bhatt23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/aistats2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A new portfolio selection strategy that adapts to a continuous side-information sequence is presented, with a universal wealth guarantee against a class of state-constant rebalanced portfolios with respect to a state function that maps each side-information symbol to a finite set of states. In particular, given that a state function belongs to a collection of functions of finite Natarajan dimension, the proposed strategy is shown to achieve, asymptotically to first order in the exponent, the same wealth as the best state-constant rebalanced portfolio with respect to the best state function, chosen in hindsight from observed market. This result can be viewed as an extension of the seminal work of Cover and Ordentlich (1996) that assumes a single state function.</p> </div> </div> </div> </li></ol> <h3 class="bibliography">2022</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="information-theory,sequential-decision-making" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://aistats.org/" rel="external nofollow noopener" target="_blank">AISTATS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/aistats2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aistats2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Bhatt--Kim2022" class="col-sm-8"> <div class="title">Parameter-Free Online Linear Optimization with Side Information via Universal Coin Betting</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt</a>, and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In AISTATS</em>, March 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a parameter-free online linear optimization algorithm that adapts to **tree-structured side information**. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.02406" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.mlr.press/v151/ryu22a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/aistats2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/olo-with-side-information" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/aistats2022-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/aistats2022-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A class of parameter-free online linear optimization algorithms is proposed that harnesses the structure of an adversarial sequence by adapting to some side information. These algorithms combine the reduction technique of Orabona and Pál (2016) for adapting coin betting algorithms for online linear optimization with universal compression techniques in information theory for incorporating sequential side information to coin betting. Concrete examples are studied in which the side information has a tree structure and consists of quantized values of the previous symbols of the adversarial sequence, including fixed-order and variable-order Markov cases. By modifying the context-tree weighting technique of Willems, Shtarkov, and Tjalkens (1995), the proposed algorithm is further refined to achieve the best performance over all adaptive algorithms with tree-structured side information of a given maximum order in a computationally efficient manner.</p> </div> </div> </div> </li></ol> <h3 class="bibliography">2021</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="operator-learning,representation-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISIT</abbr> <figure> <picture> <img src="/assets/img/publication_preview/isit2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="isit2021.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Huang--Kim2021" class="col-sm-8"> <div class="title">On the Role of Eigendecomposition in Kernel Embedding</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://scholar.google.com/citations?user=nRIpwSwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jiun-Ting Huang</a>, and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In ISIT</em>, March 2021 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose an eigendecomposition-free kernel embedding method that only requires density estimation at query points. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9517746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/isit2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/talks/isit2021-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>This paper proposes a special variant of Laplacian eigenmaps, whose solution is characterized by the underlying density and the eigenfunctions of the associated Hilbert–Schmidt operator of a similarity kernel function. In contrast to existing kernel-based spectral methods such as kernel principal component analysis and Laplacian eigenmaps, the new embedding algorithm only involves estimating density at each query point without any eigendecomposition of a matrix. A concrete example of dot-product kernels over hypersphere is provided to illustrate the applicability of the proposed framework.</p> </div> </div> </div> </li></ol> <h3 class="bibliography">2020</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="deep-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICASSP</abbr> <figure> <picture> <img src="/assets/img/publication_preview/icassp2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icassp2020.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yang--Sautiere--Ryu--Cohen2020" class="col-sm-8"> <div class="title">Feedback Recurrent Autoencoder</div> <div class="author"> <a href="https://yyang768osu.github.io/" rel="external nofollow noopener" target="_blank">Yang Yang</a>, Guillaume Sautière, <em>J. Jon Ryu</em>, and <a href="https://tacocohen.wordpress.com/" rel="external nofollow noopener" target="_blank">Taco S. Cohen</a> </div> <div class="periodical"> <em>In Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</em>, March 2020 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose a new recurrent autoencoder architecture for online compression of sequential data with temporal dependency. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1911.04018" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/document/9054074" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icassp2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this work, we propose a new recurrent autoencoder architecture, termed Feedback Recurrent AutoEncoder (FRAE), for online compression of sequential data with temporal dependency. The recurrent structure of FRAE is designed to efficiently extract the redundancy along the time dimension and allows a compact discrete representation of the data to be learned. We demonstrate its effectiveness in speech spectrogram compression. Specifically, we show that the FRAE, paired with a powerful neural vocoder, can produce high-quality speech waveforms at a low, fixed bitrate. We further show that by adding a learned prior for the latent space and using an entropy coder, we can achieve an even lower variable bitrate.</p> </div> </div> </div> </li></ol> <h3 class="bibliography">2018</h3> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="information-theory,deep-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICIP</abbr> <figure> <picture> <img src="/assets/img/publication_preview/icip2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icip2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Kim2018" class="col-sm-8"> <div class="title">Conditional distribution learning with neural networks and its application to universal image denoising</div> <div class="author"> <em>Jongha Ryu</em> and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> <em>In Proc. IEEE Int. Conf. Image Proc. (ICIP)</em>, March 2018 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We extend a simple deep-learning-extension of the information-theoretic universal denoising algorithm DUDE for structured, large-alphabet problems such as image denoising. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8451573/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icip2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/icip2018-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/icip2018-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>A simple and scalable denoising algorithm is proposed that can be applied to a wide range of source and noise models. At the core of the proposed CUDE algorithm is symbol-by-symbol universal denoising used by the celebrated DUDE algorithm, whereby the optimal estimate of the source from an unknown distribution is computed by inverting the empirical distribution of the noisy observation sequence by a deep neural network, which naturally and implicitly aggregates multi-pie contexts of similar characteristics and estimates the conditional distribution more accurately. The performance of CUDE is evaluated for grayscale images of varying bit depths, which improves upon DUDE and its recent neural network based extension, Neural DUDE.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="information-theory,statistics" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ITW</abbr> <figure> <picture> <img src="/assets/img/publication_preview/itw2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="itw2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bhatt--Huang--Kim--Ryu--Sen2018ITW" class="col-sm-8"> <div class="title">Variations on a theme by Liu, Cuff, and Verdú: The power of posterior sampling</div> <div class="author"> <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt<sup>†</sup></a>, <a href="https://scholar.google.com/citations?user=nRIpwSwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jiun-Ting Huang<sup>†</sup></a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim<sup>†</sup></a>, <em>J. Jon Ryu<sup>†</sup></em>, and Pinar Sen<sup>†</sup> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="alphabetically ordered"> </i> </div> <div class="periodical"> <em>In ITW</em>, March 2018 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We explore the power of multiple random guesses in statistical inference. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8613436" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/itw2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/talks/itw2018-slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>The Liu–Cuff–Verdu lemma states that in estimating a source X from an observation Y, making a random guess X’ from the posterior p(x|y) can go wrong at most twice as often as the optimal answer. Several variations of this fundamental, yet rather arcane, result are explored for detection, decoding, and estimation problems.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="information-theory" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Allerton</abbr> <figure> <picture> <img src="/assets/img/publication_preview/allerton2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="allerton2018.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Bhatt--Huang--Kim--Ryu--Sen2018Allerton" class="col-sm-8"> <div class="title">Monte Carlo methods for randomized likelihood decoding</div> <div class="author"> <a href="https://alankritabhatt.github.io/" rel="external nofollow noopener" target="_blank">Alankrita Bhatt<sup>†</sup></a>, <a href="https://scholar.google.com/citations?user=nRIpwSwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Jiun-Ting Huang<sup>†</sup></a>, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim<sup>†</sup></a>, <em>J. Jon Ryu<sup>†</sup></em>, and Pinar Sen<sup>†</sup> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="alphabetically ordered"> </i> </div> <div class="periodical"> <em>In Allerton</em>, March 2018 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We explore the potential of randomized decoding based on sampling from the posterior distribution via Monte Carlo techniques. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/8636049/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/allerton2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A randomized decoder that generates the message estimate according to the posterior distribution is known to achieve the reliability comparable to that of the maximum a posteriori probability decoder. With a goal of practical implementations of such a randomized decoder, several Monte Carlo techniques, such as rejection sampling, Gibbs sampling, and the Metropolis algorithm, are adapted to the problem of efficient sampling from the posterior distribution. Analytical and experimental results compare the complexity and performance of these Monte Carlo decoders for simple linear codes and the binary symmetric channel.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">Miscellaneous</h2> <h3 class="bibliography">2024</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning,divergence-matching" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <div>Tech. Report</div> </abbr> </div> <div id="Jayashankar--Ryu--Xu--Wornell2024" class="col-sm-8"> <div class="title">Lifted Residual Score Estimation</div> <div class="author"> <a href="https://tejasjayashankar.github.io/" rel="external nofollow noopener" target="_blank">Tejas Jayashankar<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, <a href="https://xiangxiangxu.mit.edu/" rel="external nofollow noopener" target="_blank">Xiangxiang Xu</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> March 2024 </div> <div class="periodical"> <a href="https://openreview.net/forum?id=zX27NDlfcu" rel="external nofollow noopener" target="_blank">An extended abstract</a> was presented at <a href="https://spigmworkshop2024.github.io/" rel="external nofollow noopener" target="_blank">ICML 2024 Workshop on Structured Probabilistic Inference &amp; Generative Modeling</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose lifted score estimation and residual score estimation, two complementary techniques that together improve score learning across VAEs, WAEs, and diffusion models. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>This paper proposes two new techniques to improve the accuracy of score estimation. The first proposal is a new objective function called the \emphlifted score estimation objective, which serves as a replacement for the score matching (SM) objective. Instead of minimizing the expected \ell_2^2-distance between the learned and true score models, the proposed objective operates in the \emphlifted space of the outer-product of a vector-valued function with itself. The distance is defined as the expected squared Frobenius norm of the difference between such matrix-valued objects induced by the learned and true score functions. The second idea is to model and learn the \emphresidual approximation error of the learned score estimator, given a base score model architecture. We empirically demonstrate that the combination of the two ideas called \emphlifted residual score estimation outperforms sliced SM in training VAE and WAE with implicit encoders, and denoising SM in training diffusion models, as evaluated by downstream metrics of sample quality such as the FID score.</p> </div> </div> </div> </li></ol> <h3 class="bibliography">2022</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="information-theory,statistics" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="Ryu--Kim2022" class="col-sm-8"> <div class="title">An Information-Theoretic Proof of Kac–Bernstein Theorem</div> <div class="author"> <em>J. Jon Ryu</em> and <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a> </div> <div class="periodical"> March 2022 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We give a short information-theoretic proof of the Kac–Bernstein characterization of the normal distribution. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.06005" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>A short, information-theoretic proof of the Kac–Bernstein theorem, which is stated as follows, is presented: For any independent random variables X and Y, if X+Y and X-Y are independent, then X and Y are normally distributed.</p> </div> </div> </div> </li></ol> <h3 class="bibliography">2017</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="probabilistic-modeling,deep-learning" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#a1a1a1"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="Yoo--Ha--Yi--Ryu--Kim--Ha--Kim--Yoon2017" class="col-sm-8"> <div class="title">Energy-based sequence GANs for recommendation and their connection to imitation learning</div> <div class="author"> Jaeyoon Yoo, Heonseok Ha, Jihun Yi, <em>Jongha Ryu</em>, Chanju Kim, Jung-Woo Ha, <a href="https://web.eng.ucsd.edu/~yhk/" rel="external nofollow noopener" target="_blank">Young-Han Kim</a>, and Sungroh Yoon </div> <div class="periodical"> March 2017 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We reinterpret EB-SeqGANs for recommendation as maximum-entropy imitation learning by viewing the energy function as a feature function. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1706.09200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Recommender systems aim to find an accurate and efficient mapping from historic data of user-preferred items to a new item that is to be liked by a user. Towards this goal, energy-based sequence generative adversarial nets (EB-SeqGANs) are adopted for recommendation by learning a generative model for the time series of user-preferred items. By recasting the energy function as the feature function, the proposed EB-SeqGANs is interpreted as an instance of maximum-entropy imitation learning.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">Theses</h2> <h3 class="bibliography">2022</h3> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="" data-selected="false"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">PhD thesis</abbr> </div> <div id="Ryu2022" class="col-sm-8"> <div class="title">From Information Theory to Machine Learning Algorithms: A Few Vignettes</div> <div class="author"> <em>Jongha Jon Ryu</em> </div> <div class="periodical"> <em>University of California San Diego</em>, September 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://escholarship.org/uc/item/5fc8x66w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This dissertation illustrates how certain information-theoretic ideas and views on learning problems can lead to new algorithms via concrete examples.The three information-theoretic strategies taken in this dissertation are (1) to abstract out the gist of a learning problem in the infinite-sample limit; (2) to reduce a learning problem into a probability estimation problem and plugging-in a "good" probability; and (3) to adapt and apply relevant results from information theory. These are applied to three topics in machine learning, including representation learning, nearest-neighbor methods, and universal information processing, where two problems are studied from each topic.</p> </div> </div> </div> </li></ol> </div> </div> <script>
document.addEventListener("DOMContentLoaded", function () {
  function getEntries() {
    return document.querySelectorAll(".pub-entry");
  }

  var checkboxes = document.querySelectorAll(".topic-filter");

  var byYearDiv  = document.getElementById("pubs-by-year");
  var byTypeDiv  = document.getElementById("pubs-by-type");
  var btnYear    = document.getElementById("btn-group-year");
  var btnType    = document.getElementById("btn-group-type");

  var btnSelected = document.getElementById("btn-show-selected");
  var btnAll      = document.getElementById("btn-show-all");

  var showOnlySelected = true;
  var currentGrouping  = "year";

  function updateGroupVisibility(root, headingSelector) {
    if (!root) return;

    // Only look at the headings we care about in this view
    var headings = root.querySelectorAll(headingSelector);

    headings.forEach(function (h) {
      var list = h.nextElementSibling;
      if (!list) return;

      // Look at the display style we set in applyFilters
      var visibleEntries = Array.prototype.filter.call(
        list.querySelectorAll(".pub-entry"),
        function (e) {
          return e.style.display !== "none";
        }
      );

      if (visibleEntries.length === 0) {
        h.style.display   = "none";
        list.style.display = "none";
      } else {
        h.style.display   = "";
        list.style.display = "";
      }
    });
  }

  function applyFilters() {
    var entries = getEntries();

    var selectedTopics = Array.prototype.slice.call(checkboxes)
      .filter(function (cb) { return cb.checked; })
      .map(function (cb) { return cb.dataset.topic; });

    entries.forEach(function (entry) {
      var kwString = (entry.dataset.keywords || "").toLowerCase();
      var kws = kwString.split(",").map(function (s) { return s.trim(); });

      var isSelected = (entry.dataset.selected === "true");

      var passesTopicFilter;
      if (selectedTopics.length === 0) {
        passesTopicFilter = true;
      } else {
        passesTopicFilter = selectedTopics.every(function (t) {
          return kws.indexOf(t) !== -1;
        });
      }

      var passesSelectedFilter = (!showOnlySelected) || isSelected;

      if (passesTopicFilter && passesSelectedFilter) {
        entry.style.display = "";
      } else {
        entry.style.display = "none";
      }
    });

    // In the "by year" view, year headings are usually <h2>
    updateGroupVisibility(byYearDiv, "h2");

    // In the "by type" view, type headings are <h2> and year headings are <h3>,
    // so we only collapse the <h3> + list pairs and leave the type headers alone.
    updateGroupVisibility(byTypeDiv, "h3");
  }

  checkboxes.forEach(function (cb) {
    cb.addEventListener("change", applyFilters);
  });

  function activateSelectedOnly() {
    showOnlySelected = true;
    if (btnSelected && btnAll) {
      btnSelected.classList.add("btn-primary", "active");
      btnSelected.classList.remove("btn-outline-primary");
      btnAll.classList.add("btn-outline-primary");
      btnAll.classList.remove("btn-primary", "active");
    }
    applyFilters();
  }

  function activateShowAll() {
    showOnlySelected = false;
    if (btnSelected && btnAll) {
      btnAll.classList.add("btn-primary", "active");
      btnAll.classList.remove("btn-outline-primary");
      btnSelected.classList.add("btn-outline-primary");
      btnSelected.classList.remove("btn-primary", "active");
    }
    applyFilters();
  }

  if (btnSelected && btnAll) {
    btnSelected.addEventListener("click", activateSelectedOnly);
    btnAll.addEventListener("click", activateShowAll);
  }

  function showByYear() {
    currentGrouping = "year";
    if (byYearDiv && byTypeDiv) {
      byYearDiv.style.display = "";
      byTypeDiv.style.display = "none";
    }
    if (btnYear && btnType) {
      btnYear.classList.add("btn-primary", "active");
      btnYear.classList.remove("btn-outline-primary");
      btnType.classList.add("btn-outline-primary");
      btnType.classList.remove("btn-primary", "active");
    }
    applyFilters();
  }

  function showByType() {
    currentGrouping = "type";
    if (byYearDiv && byTypeDiv) {
      byYearDiv.style.display = "none";
      byTypeDiv.style.display = "";
    }
    if (btnYear && btnType) {
      btnType.classList.add("btn-primary", "active");
      btnType.classList.remove("btn-outline-primary");
      btnYear.classList.add("btn-outline-primary");
      btnYear.classList.remove("btn-primary", "active");
    }
    applyFilters();
  }

  if (btnYear && btnType) {
    btnYear.addEventListener("click", showByYear);
    btnType.addEventListener("click", showByType);
  }

  // initial state
  showByYear();
  activateSelectedOnly();
});
</script> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jongha Jon Ryu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-3M2EMEPQ20"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>