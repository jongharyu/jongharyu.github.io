<!DOCTYPE html> <html lang=""> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> neural spectral methods | Jongha Jon Ryu </title> <meta name="author" content="Jongha Jon Ryu"> <meta name="description" content="# A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="Jongha Ryu, J. Jon Ryu, 류종하"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jongharyu.github.io/research/neural-spectral-methods/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Jongha <span class="font-weight-bold">Jon</span> Ryu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">neural spectral methods</h1> <p class="post-description"></p> </header> <article> <h2 id="overview">overview</h2> <p>Neural spectral methods aim to learn the dominant spectral structure of linear operators that arise across scientific and machine learning applications, including solving PDEs, analyzing nonlinear dynamics, and learning compact representations. These methods parameterize eigenfunctions with neural networks and train them using variational principles, offering scalability and flexibility beyond classical Galerkin and finite-element approaches.</p> <p>My work develops neural spectral methods through two complementary variational frameworks:</p> <ol> <li> <strong>Low-rank approximation (NestedLoRA / NeuralSVD)</strong> for compact operators <a class="citation" href="#Ryu--Xu--Erol--Bu--Zheng--Wornell2024">[1]</a>.</li> <li> <strong>Orbital minimization methods (NestedOMM)</strong> for positive-semidefinite operators <a class="citation" href="#Ryu--Zhou--Wornell2025">[2]</a>.</li> </ol> <p>Both proposals use a simple <em>nesting</em> strategy to learn ordered eigenfunctions during training, removing the need for explicit orthogonality constraints and enabling stable optimization. These methods provide improved spectral accuracy, stronger sample efficiency, and robustness across operator learning tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/neural_spectral_method-480.webp 480w,/assets/img/research/neural_spectral_method-800.webp 800w,/assets/img/research/neural_spectral_method-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/research/neural_spectral_method.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="applications">applications</h2> <p>These tools can be applied to solving PDEs as well as representation learning for reinforcement learning and graph data as demonstrated in <a class="citation" href="#Ryu--Xu--Erol--Bu--Zheng--Wornell2024">[1]</a> and <a class="citation" href="#Ryu--Zhou--Wornell2025">[2]</a>.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/neuralsvd_hydrogen_all-480.webp 480w,/assets/img/research/neuralsvd_hydrogen_all-800.webp 800w,/assets/img/research/neuralsvd_hydrogen_all-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/research/neuralsvd_hydrogen_all.png" class="img-fluid rounded z-depth-1 w-75" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> NestedLoRA learns the eigenfunctions of the Hamiltonian of the 2D hydrogen atom. </div> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/neuralsvd_sketchy_all-480.webp 480w,/assets/img/research/neuralsvd_sketchy_all-800.webp 800w,/assets/img/research/neuralsvd_sketchy_all-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/research/neuralsvd_sketchy_all.png" class="img-fluid rounded z-depth-1 w-75" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> NestedLoRA can learn structured representations for complex datasets. </div> <p>Another important application is <strong>Koopman operator learning</strong> for nonlinear dynamical systems. In <a class="citation" href="#Jeong--Ryu--Yun--Wornell2025">[3]</a>, we demonstrate that NestedLoRA (a.k.a. NeuralSVD) learns compact Koopman approximations more accurately and efficiently than VAMPnet and DPNet, enabling better long-horizon prediction and interpretable dynamical modes.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/neuralsvd_koopman_multistep-480.webp 480w,/assets/img/research/neuralsvd_koopman_multistep-800.webp 800w,/assets/img/research/neuralsvd_koopman_multistep-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/research/neuralsvd_koopman_multistep.png" class="img-fluid rounded z-depth-1 w-75" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> NestedLoRA achieves improved long-horizon prediction for stochastic dynamical systems. </div> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/neuralsvd_koopman_langevin-480.webp 480w,/assets/img/research/neuralsvd_koopman_langevin-800.webp 800w,/assets/img/research/neuralsvd_koopman_langevin-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/research/neuralsvd_koopman_langevin.png" class="img-fluid rounded z-depth-1 w-75" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> NestedLoRA learns eigenfunctions of continuous-time Langevin dynamics. </div> <h2 id="broader-perspective">broader perspective</h2> <p>Neural spectral methods sit at the interface of <strong>operator theory</strong>, <strong>numerical linear algebra</strong>, <strong>information theory</strong>, and <strong>deep learning</strong>.<br> They provide a structured approach for learning operators in scientific machine learning, and open directions include:</p> <ul> <li>extensions to higher-dimensional problems;</li> <li>convergence analysis of neural spectral methods;</li> <li>integration with uncertainty quantification;</li> <li>and spectral methods for generative and inverse modeling.</li> </ul> </article> <h2>references</h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2025b.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2025b.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Zhou--Wornell2025" class="col-sm-8"> <div class="title">Revisiting Orbital Minimization Method for Neural Operator Decomposition</div> <div class="author"> <em>J. Jon Ryu</em>, Samuel Zhou, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We revisit OMM through a modern lens, developing a scalable NestedOMM formulation that trains neural networks to decompose positive semidefinite operators. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.21952" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=AlRJVX5CRi" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2025b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/operator-omm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Spectral decomposition of linear operators plays a central role in many areas of machine learning and scientific computing. Recent work has explored training neural networks to approximate eigenfunctions of such operators, enabling scalable approaches to representation learning, dynamical systems, and partial differential equations (PDEs). In this paper, we revisit a classical optimization framework from the computational physics literature known as the *orbital minimization method* (OMM), originally proposed in the 1990s for solving eigenvalue problems in computational chemistry. We provide a simple linear algebraic proof of the consistency of the OMM objective, and reveal connections between this method and several ideas that have appeared independently across different domains. Our primary goal is to justify its broader applicability in modern learning pipelines. We adapt this framework to train neural networks to decompose positive semidefinite operators, and demonstrate its practical advantages across a range of benchmark tasks. Our results highlight how revisiting classical numerical methods through the lens of modern theory and computation can provide not only a principled approach for deploying neural networks in numerical analysis, but also effective and scalable tools for machine learning.</p> </div> </div> </div> </li> <li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/neurips2025a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="neurips2025a.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong--Ryu--Yun--Wornell2025" class="col-sm-8"> <div class="title">Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems</div> <div class="author"> <a href="https://www.linkedin.com/in/minchan-jeong-5303b7268/" rel="external nofollow noopener" target="_blank">Minchan Jeong<sup>*</sup></a>, <em>J. Jon Ryu<sup>*</sup></em>, <a href="https://fbsqkd.github.io/" rel="external nofollow noopener" target="_blank">Se-Young Yun</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In NeurIPS</em>, December 2025 </div> <div class="periodical"> </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We show that NeuralSVD (NestedLoRA) provides a scalable and stable approach for learning top-k Koopman singular functions, avoiding the unstable SVD- and inversion-based steps required by VAMPnet/DPNet. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2507.07222" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=kL2pnzClyD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/neurips2025a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/MinchanJeong/NeuralKoopmanSVD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The Koopman operator provides a principled framework for analyzing nonlinear dynamical systems through linear operator theory. Recent advances in dynamic mode decomposition (DMD) have shown that trajectory data can be used to identify dominant modes of a system in a data-driven manner. Building on this idea, deep learning methods such as VAMPnet and DPNet have been proposed to learn the leading singular subspaces of the Koopman operator. However, these methods require backpropagation through potentially numerically unstable operations on empirical second moment matrices, such as singular value decomposition and matrix inversion, during objective computation, which can introduce biased gradient estimates and hinder scalability to large systems. In this work, we propose a scalable and conceptually simple method for learning the top-k singular functions of the Koopman operator for stochastic dynamical systems based on the idea of low-rank approximation. Our approach eliminates the need for unstable linear algebraic operations and integrates easily into modern deep learning pipelines. Empirical results demonstrate that the learned singular subspaces are both reliable and effective for downstream tasks such as eigen-analysis and multi-step prediction.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row pub-entry" data-keywords="operator-learning,deep-learning,representation-learning" data-selected="true"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/icml2024a.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="icml2024a.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ryu--Xu--Erol--Bu--Zheng--Wornell2024" class="col-sm-8"> <div class="title">Operator SVD with Neural Networks via Nested Low-Rank Approximation</div> <div class="author"> <em>J. Jon Ryu</em>, <a href="https://xiangxiangxu.mit.edu/" rel="external nofollow noopener" target="_blank">Xiangxiang Xu</a>, H. S. Melichan Erol, <a href="https://buyuheng.github.io/" rel="external nofollow noopener" target="_blank">Yuheng Bu</a>, <a href="https://lizhongzheng.mit.edu/" rel="external nofollow noopener" target="_blank">Lizhong Zheng</a>, and <a href="http://allegro.mit.edu/~gww/" rel="external nofollow noopener" target="_blank">Gregory W. Wornell</a> </div> <div class="periodical"> <em>In ICML</em>, July 2024 </div> <div class="periodical"> <a href="https://ml4physicalsciences.github.io/2023/files/NeurIPS_ML4PS_2023_225.pdf" rel="external nofollow noopener" target="_blank">An extended abstract</a> was presented at <a href="https://ml4physicalsciences.github.io/2023/" rel="external nofollow noopener" target="_blank">Machine Learning and the Physical Sciences Workshop, NeurIPS 2023</a>. </div> <div class="links"> <div class="tldr"> <strong>TL;DR:</strong> We propose an efficient unconstrained nested optimization framework for computing the leading singular values and singular functions of a linear operator using neural networks. </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.03655" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=qESG5HaaoJ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/icml2024a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/jongharyu/neural-svd" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/icml2024-neuralsvd-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/talks/ita2024-neuralsvd.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-L singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cases in computational physics and machine learning.</p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jongha Jon Ryu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 16, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-3M2EMEPQ20"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>